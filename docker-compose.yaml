services:
  base:
    build:
      context: .
      dockerfile: Dockerfile.base
    image: langchain-rag-base:latest

  redis:
    image: redis/redis-stack:latest
    command: redis-stack-server /usr/local/etc/redis/redis.conf
    ports:
      - "6379:6379"
      - "8001:8001"  # RedisInsight web interface
    volumes:
      - redis_data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - rag-network

  neo4j:
    image: neo4j:5.15-community
    container_name: neo4j-mindmap
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/mindmapneo4j
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_memory_heap_initial__size=1G
      - NEO4J_dbms_memory_heap_max__size=2G
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -O /dev/null http://localhost:7474 || exit 1"]
      interval: 5s
      timeout: 10s
      retries: 10
    networks:
      - rag-network

  app:
    build: .
    container_name: langchain-app
    ports:
      - "8000:8000"
    depends_on:
      base:
        condition: service_started
      redis:
        condition: service_healthy
    environment:
      # Redis configuration
      - REDIS_URL=redis://redis:6379
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      
      # --- LLM Provider ---
      # Set to "openrouter" or "ollama"
      - LLM_PROVIDER=${LLM_PROVIDER:-openrouter}

      # --- OpenRouter/OpenAI API Configuration ---
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_API_BASE=${OPENAI_API_BASE:-https://openrouter.ai/api/v1}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-deepseek/deepseek-r1:free}

      # --- Ollama Configuration ---
      # Use host.docker.internal to connect to Ollama running on the host machine
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:3b}
      
      # LangSmith configuration (optional)
      - LANGSMITH_ENDPOINT=${LANGSMITH_ENDPOINT}
      - LANGSMITH_PROJECT=${LANGSMITH_PROJECT}
      - LANGSMITH_TRACING=${LANGSMITH_TRACING}
      - LANGSMITH_API_KEY=${LANGSMITH_API_KEY}
      
      # Application configuration
      - DEFAULT_REFERER=${DEFAULT_REFERER:-https://localhost:8000}
      - DEFAULT_TITLE=${DEFAULT_TITLE:-LangChain RAG API}
      
      # Debug configuration
      - DEBUG=true
      
    volumes:
      - .:/app
      - ./certs:/app/certs  # SSL certificates
    restart: unless-stopped
    networks:
      - rag-network

  mindmap:
    build:
      context: .
      dockerfile: ./mindmap_service/Dockerfile
    container_name: mindmap-service
    ports:
      - "8003:8003"
    depends_on:
      base:
        condition: service_started
      neo4j:
        condition: service_healthy
      redis:
        condition: service_healthy
      app:
        condition: service_started
    environment:
      # Neo4j configuration
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=mindmapneo4j
      - NEO4J_DATABASE=neo4j
      
      # Redis configuration (use different DB than main app)
      - REDIS_URL=redis://redis:6379
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      
      # RAG Service configuration
      - RAG_SERVICE_URL=http://app:8000/api
      
      # Mind map service configuration
      - DEBUG=${DEBUG:-false}
      - MAX_CONCURRENT_JOBS=5
      - BATCH_RESULT_TTL=3600
    volumes:
      - ./mindmap_service:/app
    restart: unless-stopped
    networks:
      - rag-network

  web-ui:
    image: nginx:latest
    ports:
      - "8080:80"
    volumes:
      - ./web_ui:/usr/share/nginx/html
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - app
    restart: unless-stopped
    networks:
      - rag-network

  transcoder:
    build:
      context: .
      dockerfile: ./transcoder/Dockerfile
    container_name: transcoder-app
    ports:
      - "8002:5001"
    depends_on:
      - base
    volumes:
      - transcoder_uploads:/home/appuser/app/uploads
    environment:
      # Transcoder configuration
      - SUPPORTED_EXTENSIONS=pdf,docx,md,markdown
      - MAX_FILE_SIZE_MB=50
      - REQUEST_TIMEOUT_SECONDS=30
    restart: unless-stopped
    networks:
      - rag-network

volumes:
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/redis
  neo4j_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/neo4j
  neo4j_logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./data/neo4j_logs
  transcoder_uploads:
    driver: local

networks:
  rag-network:
    driver: bridge